{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c01d229-b690-4d43-9356-b3e03026e0d7",
   "metadata": {},
   "source": [
    "## 🔧 1. Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6fc617-0580-40e4-a132-84a4aa81fba6",
   "metadata": {},
   "source": [
    "You can install PySpark via pip:\n",
    "```\n",
    "pip install pyspark\n",
    "```\n",
    "To verify the installation, run the following command in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4045cab8-f55b-4a00-8728-e54fef9c99d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.4\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "print(pyspark.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873eef98-5769-466a-81f0-ce35a7eada6b",
   "metadata": {},
   "source": [
    "Before intializing PySpark, please make sure your computer installed Java 8 or later, but Java 9+ can sometimes cause issues. To check your Java version, run:\n",
    "```{sh}\n",
    "java -version\n",
    "```\n",
    "You should see something like:\n",
    "```\n",
    "java version \"1.8.0_281\"\n",
    "```\n",
    "If Java is not installed, install it trough the link\n",
    "-  MacBook: [Install Java on MacBook](https://www.java.com/en/download/) (Check if your computer has Intel or Apple M CPU first)\n",
    "-  Windows: [Install Java on PC](https://www.java.com/download/ie_manual.jsp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4bb245-6e8a-4990-9472-ecfcf9ae9bbf",
   "metadata": {},
   "source": [
    "## 🚀 2. Start a Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7acef92e-5093-4859-9ca5-c91af793a9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/18 23:15:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark_Tutorial\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fac4be-ec79-4e26-8a1a-9d8c80e64672",
   "metadata": {},
   "source": [
    "- `SparkSession.builder`: it starts building a new SparkSession configuration; you may treat it as a fluent API (a builder pattern) for customizing your Spark app.\n",
    "- `.appName(\"PySpark_Tutorial\")`: it sets the name of your Spark application\n",
    "- `.master(\"local[*]\")`: `\"local\"` tells Spark to run locally, not on a cluster; `\"*\"` tells Spark to use all available CPU cores on your machine.`\n",
    "- `.getOrCreate()`: Creates a new SparkSession or returns an existing one if it already exists."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40ef85b-ad74-4d48-a7c6-6d4c0bb3a77f",
   "metadata": {},
   "source": [
    "## 📂 3. Load and Explore Data\n",
    "The data required for this example is saved at s3://de300spring2025/dinglin_xia/lab7_data/adult.csv.\n",
    "\n",
    "Assuming your file is named \"adult.csv\" and contains a column \"filtered\" (JSON array of words):\n",
    "We’ll walk through several important steps:\n",
    "\n",
    "1. Feature Engineering\n",
    "\n",
    "2. Bias Analysis on Marital Status\n",
    "\n",
    "3. Joining with Supplemental Gender Data\n",
    "\n",
    "4. Exporting Final Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbe3712-567a-46be-84cc-66dccc795810",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql.functions import col, when, isnan, isnull, count, avg, trim\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0feb1a8-a60d-4b3f-9c1c-4b9dcbf8c1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"data\"\n",
    "# source https://www.statista.com/statistics/242030/marital-status-of-the-us-population-by-sex/\n",
    "# the first value is male and the second is for female\n",
    "MARITAL_STATUS_BY_GENDER = [\n",
    "    [\"Never-married\", 47.35, 41.81],\n",
    "    [\"Married-AF-spouse\", 67.54, 68.33],\n",
    "    [\"Widowed\", 3.58, 11.61],\n",
    "    [\"Divorced\", 10.82, 15.09]\n",
    "]\n",
    "MARITAL_STATUS_BY_GENDER_COLUMNS = [\"marital_status_statistics\", \"male\", \"female\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f82fc5d-9e0a-41db-9089-b2fbbe5286d8",
   "metadata": {},
   "source": [
    "### 📁 spark.read\n",
    "- `schema(schema)`: Tells Spark to use your predefined schema.\n",
    "\n",
    "- `option(\"header\", \"false\")`: There’s no header row in the CSV file.\n",
    "\n",
    "- `option(\"inferSchema\", \"false\")`: You don’t want Spark to guess column types.\n",
    "\n",
    "- `csv(...)`: Loads the CSV files from the given folder (wildcard *.csv means all CSVs).\n",
    "\n",
    "### 🧵 Repartition\n",
    "Repartitions the DataFrame into 8 partitions, which improves parallel processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001d819f-2cc6-4342-b7a8-ee3fe21565c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(spark: SparkSession) -> DataFrame:\n",
    "    \"\"\"\n",
    "    read data based on the given schema; this is much faster than spark determining the schema\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the schema for the dataset\n",
    "    # It tells Spark: 1. what column name to expect 2. what data type it should have 3. and whether it can contain null\n",
    "    # It's faster than letting Spark infer types.\n",
    "    schema = StructType([\n",
    "        StructField(\"age\", IntegerType(), True),\n",
    "        StructField(\"workclass\", StringType(), True),\n",
    "        StructField(\"fnlwgt\", FloatType(), True),\n",
    "        StructField(\"education\", StringType(), True),\n",
    "        StructField(\"education_num\", FloatType(), True),\n",
    "        StructField(\"marital_status\", StringType(), True),\n",
    "        StructField(\"occupation\", StringType(), True),\n",
    "        StructField(\"relationship\", StringType(), True),\n",
    "        StructField(\"race\", StringType(), True),\n",
    "        StructField(\"sex\", StringType(), True),\n",
    "        StructField(\"capital_gain\", FloatType(), True),\n",
    "        StructField(\"capital_loss\", FloatType(), True),\n",
    "        StructField(\"hours_per_week\", FloatType(), True),\n",
    "        StructField(\"native_country\", StringType(), True),\n",
    "        StructField(\"income\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "    # Read the dataset\n",
    "    data = spark.read \\\n",
    "        .schema(schema) \\\n",
    "        .option(\"header\", \"false\") \\\n",
    "        .option(\"inferSchema\", \"false\") \\\n",
    "        .csv(os.path.join(DATA_FOLDER,\"*.csv\")) \n",
    "\n",
    "    data = data.repartition(8)\n",
    "\n",
    "    # This loop finds all columns with FloatType and casts them to IntegerType\n",
    "    float_columns = [f.name for f in data.schema.fields if isinstance(f.dataType, FloatType)]\n",
    "    for v in float_columns:\n",
    "        data = data.withColumn(v, data[v].cast(IntegerType()))\n",
    "\n",
    "    # Get the names of all StringType columns\n",
    "    string_columns = [f.name for f in data.schema.fields if isinstance(f.dataType, StringType)]\n",
    "\n",
    "    # Remove leading and trailing spaces in all string columns\n",
    "    for column in string_columns:\n",
    "        data = data.withColumn(column, trim(data[column])) # Applies the trim() function to remove leading/trailing spaces, which is a common issue in raw CSVs.\n",
    "\n",
    "    # Show the first 5 rows of the dataset\n",
    "    data.show(5)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd279c5-c761-4af5-83b2-11d6633920e8",
   "metadata": {},
   "source": [
    "🔧 Breakdown of the query to count missing value\n",
    "- `data.columns`: List of all column names.\n",
    "\n",
    "- `isnan(c)`: Checks if a value is NaN (only for numeric types).\n",
    "\n",
    "- `isnull(c)`: Checks if a value is null (works for any type).\n",
    "\n",
    "- `when(condition, value)`: Creates a conditional column.\n",
    "\n",
    "- `count(...)`: Counts how many times that condition is met.\n",
    "\n",
    "- `.alias(c)`: Names the result column after the original column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00d1148-ba19-4e77-9e11-9d78b0af2124",
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_values(data: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    count the number of samples with missing values for each row\n",
    "    remove such samples\n",
    "    \"\"\"\n",
    "    # Count Missing Values per Column\n",
    "    missing_values = data.select([count(when(isnan(c) | isnull(c), c)).alias(c) for c in data.columns])\n",
    "\n",
    "    # Show the missing values count per column\n",
    "    missing_values.show()\n",
    "\n",
    "    # Get the number of samples in the DataFrame\n",
    "    num_samples = data.count()\n",
    "\n",
    "    # Print the number of samples\n",
    "    print(\"Number of samples:\", num_samples)  \n",
    "\n",
    "    data = data.dropna()      \n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e17741-06c7-47a6-b399-e7457604feda",
   "metadata": {},
   "source": [
    "### 📌 Feature Engineering Function\n",
    "📖 Explanation:\n",
    "This function programmatically finds all integer features and creates new features by multiplying each pair. It helps capture interaction effects for models or exploratory analysis.\n",
    "\n",
    "- `withColumn()` creates a new column in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2cf776-3fd6-45a7-8420-b4a34f280c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(data: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate the product of each pair of integer features\n",
    "    \"\"\"\n",
    "    # Identify all integer-type columns in the dataset\n",
    "    integer_columns = [f.name for f in data.schema.fields if isinstance(f.dataType, IntegerType)]\n",
    "    \n",
    "    # For each pair of integer columns, compute a new column that is their product\n",
    "    for i, col1 in enumerate(integer_columns):\n",
    "        for col2 in integer_columns[i:]:  # Avoid duplicate pairs\n",
    "            product_col_name = f\"{col1}_x_{col2}\"\n",
    "            data = data.withColumn(product_col_name, col(col1) * col(col2))\n",
    "    \n",
    "    # Preview first 5 rows to check new columns\n",
    "    data.show(5)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126bc286-d8a4-43c8-9622-f77f130de689",
   "metadata": {},
   "source": [
    "### 📌 Analyze Bias by Marital Status\n",
    "📖 Explanation:\n",
    "This function investigates if capital_gain varies significantly with marital_status. It also filters and inspects a specific subgroup — divorced individuals — to facilitate further analysis or visualization.\n",
    "Compute Average Capital Gain by Marital Status\n",
    "- `groupBy(\"marital_status\")`: Groups the dataset by marital status (e.g., \"Married\", \"Single\", \"Divorced\").\n",
    "\n",
    "- `agg(...)`: Performs an aggregation, in this case avg(\"capital_gain\").\n",
    "\n",
    "- `alias(...)`: Renames the resulting column to \"average_capital_gain\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7fe919-a492-4ab1-b81a-8ed349da2922",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_marital_status(data: DataFrame):\n",
    "    \"\"\"\n",
    "    Analyze if there's a bias in capital gain by marital status\n",
    "    \"\"\"\n",
    "    # Group by marital status and compute average capital gain\n",
    "    average_capital_gain = data.groupBy(\"marital_status\").agg(avg(\"capital_gain\").alias(\"average_capital_gain\"))\n",
    "    average_capital_gain.show()\n",
    "\n",
    "    # Filter only rows with marital_status == \"Divorced\"\n",
    "    divorced_data = data.filter(data.marital_status == \"Divorced\")\n",
    "    divorced_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1903cc2d-0257-47f1-98be-30d4a7efd81c",
   "metadata": {},
   "source": [
    "### 📌 Join with External Gender Statistics\n",
    "📖 Explanation:\n",
    "This function enriches the dataset by joining it with an external dataset containing U.S. gender distribution by marital status. The `outer` join ensures we keep unmatched records for completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d546c6-97a3-48da-99ce-4fa7bab4e468",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_with_US_gender(spark: SparkSession, data: DataFrame):\n",
    "    \"\"\"\n",
    "    Join with external data about marital status statistics by gender\n",
    "    \"\"\"\n",
    "    # Example data (assumed predefined in MARITAL_STATUS_BY_GENDER & *_COLUMNS)\n",
    "    us_df = spark.createDataFrame(MARITAL_STATUS_BY_GENDER, MARITAL_STATUS_BY_GENDER_COLUMNS)\n",
    "\n",
    "    # Outer join on marital_status\n",
    "    return data.join(us_df, data.marital_status == us_df.marital_status_statistics, 'outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2822d331-8057-430a-88e7-0e8a0fbddd9b",
   "metadata": {},
   "source": [
    "### ✅ Main Pipeline\n",
    "📖 Explanation:\n",
    "This function ties all preprocessing steps together and writes the result to a CSV file. You can run it with main() in a PySpark environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb5e52c-cece-495e-9644-87a2de0a8b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    spark = SparkSession.builder.appName(\"Read Adult Dataset\").getOrCreate()\n",
    "    \n",
    "    data = read_data(spark)               # Load dataset\n",
    "    data = missing_values(data)           # Handle missing values (assumed implemented)\n",
    "    data = feature_engineering(data)      # Add interaction features\n",
    "    bias_marital_status(data)             # Analyze capital gain by marital status\n",
    "    data = join_with_US_gender(spark, data)  # Enrich with gender statistics\n",
    "    \n",
    "    data.show(5)                          # Preview final data\n",
    "    data.write.format('csv').option('header', 'true').mode('overwrite').save('saved.csv')  # Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa374bbe-e287-4e36-b9ee-758deeec36a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39619e2-5dfa-4671-9ffc-0ce9b8b8c90e",
   "metadata": {},
   "source": [
    "## Lab Assignment\n",
    "### 1.  Spark-sql\n",
    "Add one more cell in to select rows with 'age' between 30 and 50 (inclusive) and transforms the selected pyspark dataframe into pandas dataframe and print out the summary statistics using 'describe()'. Some useful methods are:\n",
    "- `filter(...)`: filters Spark DataFrame rows using boolean conditions\n",
    "- `.toPandas()`: collects the Spark DataFrame into memory, so only use it on small to medium datasets\n",
    "- `.describe()`: gives summary stats (mean, std, min, max, quartiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe76370-7067-4298-a6f0-179c17091339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_age_range(data: DataFrame, lower: int = 30, upper: int = 50):\n",
    "    \"\"\"\n",
    "    Filters rows where age is between `lower` and `upper` (inclusive),\n",
    "    converts to pandas DataFrame, and prints summary statistics.\n",
    "    \"\"\"\n",
    "    # Filter Spark DataFrame\n",
    "\n",
    "    # Convert to pandas\n",
    "\n",
    "    # Print summary statistics\n",
    "    print(f\"Summary statistics for age between {lower} and {upper}:\")\n",
    "\n",
    "    return pandas_df  # Optional: return if you want to use it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5140ce68-45d8-42ca-84c1-2d4d23397dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    spark = SparkSession.builder.appName(\"Read Adult Dataset\").getOrCreate()\n",
    "    \n",
    "    data = read_data(spark)               # Load dataset\n",
    "    data = missing_values(data)           # Handle missing values (assumed implemented)\n",
    "    data = feature_engineering(data)      # Add interaction features\n",
    "    bias_marital_status(data)             # Analyze capital gain by marital status\n",
    "    data = join_with_US_gender(spark, data)  # Enrich with gender statistics\n",
    "\n",
    "    data_df = describe_age_range(data)    # Select and Transform\n",
    "     \n",
    "    data.show(5)                          # Preview final data\n",
    "    data.write.format('csv').option('header', 'true').mode('overwrite').save('saved.csv')  # Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61eebf4c-e389-4d93-96a0-65e64bf19774",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d57e78-917d-47db-b43e-081f6a24ac2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4e1eca-854e-40f5-a4bd-1d0e3a58308a",
   "metadata": {},
   "source": [
    "### 2. Word Count\n",
    "Save only the words that have count greater or equal to 3.\n",
    "\n",
    "#### 1. Setup Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e7fa0a-4769-4129-8288-ae2a2d4d33f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"WordCount\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b8ca0c-bbdf-4b25-8f8f-2aefb008dabb",
   "metadata": {},
   "source": [
    "#### 2. Read Text File\n",
    "Fill in your text file direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670d34cc-1161-48b9-8591-62b9ca1c3598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each line becomes a row in the DataFrame\n",
    "lines = spark.read.text(\"\")\n",
    "lines.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdcfc25-a2d0-4556-a12d-75cc596c040d",
   "metadata": {},
   "source": [
    "#### 3. Split Lines into Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665e6ee9-99eb-4805-a341-8d59c1ee5b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, explode, col\n",
    "\n",
    "# Split lines into arrays of words, then explode into one word per row\n",
    "words = lines.select(explode(split(col(\"value\"), \" \")).alias(\"word\"))\n",
    "words.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facb5fb9-e57c-4cb3-95b4-8ac2e0134245",
   "metadata": {},
   "source": [
    "#### 4. Count Word Frequencies\n",
    "Fill in the column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33c4dc0-767c-4c64-9e8a-02dda36bcd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = words.groupBy(\"column_name\").count()\n",
    "word_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6040d78-192a-47b1-8736-fa885b9d2ce6",
   "metadata": {},
   "source": [
    "#### 5. Filter Rare Words\n",
    "Fill in the condition in `filter()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae83ec34-9b09-438e-adba-0517f8a2d1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts_filtered = word_counts.filter()\n",
    "word_counts_filtered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a15fa57-a452-4f61-b333-65ad0cdce681",
   "metadata": {},
   "source": [
    "#### 6. Stop the Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f280ef-676e-4da7-815d-6c8f1cc54ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
