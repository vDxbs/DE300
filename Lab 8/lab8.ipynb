{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "278938fe-922e-499c-8b02-74cff3d18f4d",
   "metadata": {},
   "source": [
    "# Apache Airflow Tutorial\n",
    "⚠️ Cost-Saving Note\n",
    "Amazon MWAA can be expensive to run. To minimize cost, please form a temporary group of two or more students (this group may differ from your homework group).\n",
    "\n",
    "## Setting Up Airflow on AWS (Amazon MWAA)\n",
    "0. Create an S3 bucket\n",
    "\n",
    "    - Go to the AWS Management Console and ensure your region is set to N. Virginia.\n",
    "\n",
    "    - Create a new bucket named Lastname1-Lastname2-Bucket (replace with actual last names).\n",
    "\n",
    "    - Leave all other settings as default.\n",
    "\n",
    "    - After the bucket is created, create a folder named dags inside it.\n",
    "\n",
    "1. Open Amazon MWAA\n",
    "\n",
    "    - In the AWS Console search bar, type Amazon MWAA and navigate to the service.\n",
    "\n",
    "    - Make sure the region is N. Virginia, then click \"Create environment\".\n",
    "\n",
    "2. Configure the environment\n",
    "\n",
    "    - Set the environment name to Lastname1-Lastname2-MWAA.\n",
    "\n",
    "    - Select the latest Airflow version available.\n",
    "\n",
    "3. Configure storage\n",
    "\n",
    "    - For the S3 bucket, choose the one you just created.\n",
    "\n",
    "    - Set the DAGs folder to the dags folder you created inside that bucket.\n",
    "\n",
    "4. Configure networking\n",
    "\n",
    "    - For VPC, select MWAA-VPC-DE300.\n",
    "\n",
    "    - For Web server access, choose Public.\n",
    "\n",
    "5. Launch the environment\n",
    "\n",
    "    - Scroll to the bottom of the page and click \"Create environment\".\n",
    "\n",
    "    - It will take approximately 20–30 minutes for the environment to become active.\n",
    "\n",
    "## Introduction\n",
    "Apache Airflow is an open-source workflow orchestration tool that allows users to define, schedule, and monitor workflows as Directed Acyclic Graphs (DAGs). It is widely used for automating ETL (Extract, Transform, Load) processes, data pipelines, and machine learning workflows.\n",
    "\n",
    "This tutorial will guide you through the installation, basic concepts, and workflow creation using Airflow.\n",
    "\n",
    "## Installation\n",
    "Before installing Airflow, ensure you have the latest version pip. You can upgrade it by\n",
    "```\n",
    "python -m pip install --upgrade pip\n",
    "```\n",
    "The we can install Airflow by\n",
    "```\n",
    "pip install apache-airflow\n",
    "```\n",
    "In the examples in the lab session today, we may also need the following 2 dependencies:\n",
    "```\n",
    "pip install apache-airflow-providers-sftp\n",
    "    - [ ] pip install apache-airflow-providers-postgres\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb12a36-3139-489d-aa04-f4626c516bbf",
   "metadata": {},
   "source": [
    "## Concepts\n",
    "### DAG (Directed Acyclic Graph)\n",
    "\n",
    "A Directed Acyclic Graph (DAG) is a collection of tasks that are organized in a way that reflects their dependencies. In Airflow, DAGs define workflows where tasks must be executed in a specified sequence without any cyclic dependencies (loops). Each DAG has a start date, a schedule interval, and a set of tasks connected by dependencies.\n",
    "\n",
    "Key characteristics of DAGs:\n",
    "\n",
    "    - Directed: Tasks execute in a specific order.\n",
    "\n",
    "    - Acyclic: No task can depend on a downstream task that eventually depends on itself, preventing infinite loops.\n",
    "\n",
    "    - Graph: A network structure that allows for complex task dependencies.\n",
    "\n",
    "Example DAG visualization:\n",
    "\n",
    "### Tasks and Operators\n",
    "\n",
    "Tasks are the fundamental building blocks of a DAG. Airflow provides predefined Operators for various tasks, such as:\n",
    "\n",
    "    - `PythonOperator` – Runs Python functions\n",
    "\n",
    "    - `BashOperator` – Executes shell commands\n",
    "\n",
    "    - `PostgresOperator` – Runs SQL queries on a Postgres database\n",
    "\n",
    "    - `SFTPOperator` – Transfers files via SFTP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642e23a0-6ac2-4edd-8a28-32bfe63f1010",
   "metadata": {},
   "source": [
    "## A Simple DAGs example\n",
    "#### 0.Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b650fb2-50d6-4346-aded-a3d6042eeea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator, BranchPythonOperator\n",
    "from airflow.operators.empty import EmptyOperator\n",
    "import pendulum\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61e6061-9799-4c38-b87c-e10aab7dfc9b",
   "metadata": {},
   "source": [
    "#### 1.Define the workflow schedule frequency\n",
    "cheduling: Airflow supports cron-like scheduling using `schedule_interval` (e.g., `@daily`, `@hourly`, `0 12 * * *`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037a6dba-b9d0-4eaa-b086-ee4df8a27419",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKFLOW_SCHEDULE = \"@hourly\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b531c4-749f-483b-8860-38de16e3d0d6",
   "metadata": {},
   "source": [
    "#### 2.Default arguments dictionary for the DAG execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e492a3-9bbc-4590-a411-dcad3c89c1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default arguments dictionary for the DAG execution\n",
    "default_args = {\n",
    "    'owner': 'johndoe',  # Owner of the DAG\n",
    "    'depends_on_past': False,  # Ensures tasks do not depend on past runs\n",
    "    'start_date': pendulum.today('UTC').add(days=-1),  # DAG start date (yesterday)\n",
    "    'retries': 1,  # Number of retry attempts upon failure\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c515e6-1d7d-41c7-b74e-b19c4951a281",
   "metadata": {},
   "source": [
    "#### 3.Task Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c07174-8546-4967-9ed1-87435ff793c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: Generates a success/failure outcome randomly\n",
    "def task1_func(**kwargs):\n",
    "    \"\"\"\n",
    "    Simulates a task that randomly determines success or failure.\n",
    "    Output:\n",
    "        - 'success' (50% probability)\n",
    "        - 'failure' (50% probability)\n",
    "    \"\"\"\n",
    "    print('Running task 1')\n",
    "    value = 'success' if random.random() < 0.5 else 'failure'\n",
    "    print(f'Task 1 output: {value}')\n",
    "    return {'status': value}\n",
    "\n",
    "# Task 2: Generates a random integer between 0 and 10\n",
    "def task2_func(**kwargs):\n",
    "    \"\"\"\n",
    "    Simulates a task that generates a random integer in the range [0, 10].\n",
    "    Output:\n",
    "        - Random integer (0-10)\n",
    "    \"\"\"\n",
    "    print('Running task 2')\n",
    "    value = random.randint(0, 10)\n",
    "    print(f'Task 2 output: {value}')\n",
    "    return {'value': value}\n",
    "\n",
    "# Task 3: Generates two random integers between 0 and 10\n",
    "def task3_func(**kwargs):\n",
    "    \"\"\"\n",
    "    Simulates a task that generates two random integers in the range [0, 10].\n",
    "    Output:\n",
    "        - Two random integers (0-10)\n",
    "    \"\"\"\n",
    "    print('Running task 3')\n",
    "    value_one = random.randint(0, 10)\n",
    "    value_two = random.randint(0, 10)\n",
    "    print(f'Task 3 output: {value_one} {value_two}')\n",
    "    return {'value1': value_one, 'value2': value_two}\n",
    "\n",
    "# Task 4: Aggregates results from Task 1, Task 2, and Task 3\n",
    "def task4_func(**kwargs):\n",
    "    \"\"\"\n",
    "    Collects results from task1, task2, and task3 using XCom.\n",
    "    Decides whether task5 should be executed based on:\n",
    "        - If task3's first value is greater than the second value\n",
    "        - If task1 returned 'success'\n",
    "    Output:\n",
    "        - \"do-task5\" if conditions are met, else empty string\n",
    "    \"\"\"\n",
    "    print('Running task 4')\n",
    "    ti = kwargs['ti']\n",
    "    task1_return_value = ti.xcom_pull(task_ids='task1')\n",
    "    task2_return_value = ti.xcom_pull(task_ids='task2')\n",
    "    task3_return_value = ti.xcom_pull(task_ids='task3')\n",
    "    \n",
    "    print(\"Task 1 returned: \", task1_return_value)\n",
    "    print(\"Task 2 returned: \", task2_return_value)\n",
    "    print(\"Task 3 returned: \", task3_return_value)\n",
    "\n",
    "    return_value = \"\"\n",
    "    if task3_return_value['value1'] > task3_return_value['value2'] and task1_return_value['status'] == \"success\":\n",
    "        return_value = \"do-task5\"\n",
    "    \n",
    "    return return_value\n",
    "\n",
    "# Branching decision function\n",
    "def decide_which_path(**kwargs):\n",
    "    \"\"\"\n",
    "    Determines the next task to execute based on task4's output.\n",
    "    Output:\n",
    "        - 'task5' if task4 returns \"do-task5\"\n",
    "        - 'dummy_task' otherwise\n",
    "    \"\"\"\n",
    "    ti = kwargs['ti']\n",
    "    task4_return_value = ti.xcom_pull(task_ids='task4')\n",
    "    if task4_return_value == \"do-task5\":\n",
    "        return 'task5'\n",
    "    else:\n",
    "        return 'dummy_task'\n",
    "\n",
    "# Task 5: Final task execution\n",
    "def task5_func(**kwargs):\n",
    "    \"\"\"\n",
    "    Executes task5 if the branching condition is met.\n",
    "    Output:\n",
    "        - A dictionary indicating task completion\n",
    "    \"\"\"\n",
    "    print('Running task 5')\n",
    "    return {'task': 'task5', 'status': 'completed'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd19f6a0-3787-4ab3-837e-4a1ddba7ec82",
   "metadata": {},
   "source": [
    "#### 4.Initiate the DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff188b64-d21a-4f04-a993-34abc73fe099",
   "metadata": {},
   "outputs": [],
   "source": [
    "dag = DAG(\n",
    "    'example_dag',  # Name of the DAG\n",
    "    default_args=default_args,\n",
    "    description='An example DAG with dependencies',\n",
    "    schedule=WORKFLOW_SCHEDULE,  # Schedule interval for DAG execution\n",
    "    tags=[\"de300\"]  # DAG tagging for categorization\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0134341d-7f24-4750-bc9d-1be4dadeff7f",
   "metadata": {},
   "source": [
    "#### 5.Define the tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70750678-9927-46be-9fe9-5f8a1d3e807f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: Generates success/failure\n",
    "task1 = PythonOperator(\n",
    "    task_id='task1',\n",
    "    python_callable=task1_func,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Task 2: Generates a random integer\n",
    "task2 = PythonOperator(\n",
    "    task_id='task2',\n",
    "    python_callable=task2_func,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Task 3: Generates two random integers\n",
    "task3 = PythonOperator(\n",
    "    task_id='task3',\n",
    "    python_callable=task3_func,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Task 4: Aggregates task results and makes a decision\n",
    "task4 = PythonOperator(\n",
    "    task_id='task4',\n",
    "    python_callable=task4_func,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Branching task: Determines execution path based on task4's output\n",
    "decide = BranchPythonOperator(\n",
    "    task_id='branch_task',\n",
    "    python_callable=decide_which_path,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Task 5: Final task if the condition is met\n",
    "task5 = PythonOperator(\n",
    "    task_id='task5',\n",
    "    python_callable=task5_func,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Dummy task: Acts as an alternative path\n",
    "dummy_task = EmptyOperator(\n",
    "    task_id='dummy_task',\n",
    "    dag=dag,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b9d02a-9802-4a4a-9791-837118cdf30d",
   "metadata": {},
   "source": [
    "#### 6.Define the dependencies among tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6f7b3d-543f-4305-8587-2a99c9b6e4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "[task1, task2, task3] >> task4 >> decide  # Task1, Task2, Task3 -> Task4 -> Decision\n",
    "\n",
    "decide >> task5  # If condition met, execute Task5\n",
    "decide >> dummy_task  # If condition not met, execute Dummy Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008db1dc-5227-4cf5-9bfa-8e0285de8b90",
   "metadata": {},
   "source": [
    "### Best Practices\n",
    "\n",
    "1. Parameterize workflows: Use Airflow variables and templates.\n",
    "\n",
    "2. Use task dependencies: Define dependencies using `>>` and `<<`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5b141a-d277-4a8b-98e7-970113bd59e5",
   "metadata": {},
   "source": [
    "## 0. Reads parameters from a TOML configuration file (optional)\n",
    "```{python}\n",
    "def read_config() -> dict:\n",
    "    path = pathlib.Path(CONFIG_FILE)\n",
    "    with path.open(mode=\"rb\") as param_file:\n",
    "        params = tomli.load(param_file)\n",
    "    return params\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd8022c-2ab0-4424-abf2-3958919ffc47",
   "metadata": {},
   "source": [
    "## 1. DAG Definition and Configuration\n",
    "    - `dag_id`: Unique identifier for the DAG.\n",
    "    - `default_args`: Dictionary with default parameters for tasks.\n",
    "    - `schedule_interval`: Defines how often the DAG runs.\n",
    "    - `start_date`: Determines when the DAG starts running.\n",
    "    - `tags`: Helps categorize DAGs in the UI.\n",
    "    \n",
    "```python\n",
    "from airflow import DAG\n",
    "import pendulum\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'user',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': pendulum.today('UTC').add(days=-1),\n",
    "    'retries': 1,\n",
    "}\n",
    "\n",
    "dag = DAG(\n",
    "    'example_dag',\n",
    "    default_args=default_args,\n",
    "    description='A simple DAG example',\n",
    "    schedule_interval=\"@daily\",\n",
    "    tags=['example']\n",
    ")\n",
    "```\n",
    "\n",
    "## 2. Task Definition\n",
    "### PythonOperator\n",
    "Usage: Executes a Python function within a DAG.\n",
    "```python\n",
    "from airflow.operators.python import PythonOperator\n",
    "\n",
    "def my_function(**kwargs):\n",
    "    print(\"Hello from PythonOperator\")\n",
    "\n",
    "task = PythonOperator(\n",
    "    task_id='print_hello',\n",
    "    python_callable=my_function,\n",
    "    dag=dag\n",
    ")\n",
    "```\n",
    "### BashOperator\n",
    "```python\n",
    "from airflow.operators.bash import BashOperator\n",
    "\n",
    "task = BashOperator(\n",
    "    task_id='print_date',\n",
    "    bash_command='date',\n",
    "    dag=dag\n",
    ")\n",
    "```\n",
    "### EmptyOperator\n",
    "```python\n",
    "from airflow.operators.empty import EmptyOperator\n",
    "\n",
    "start = EmptyOperator(task_id='start', dag=dag)\n",
    "end = EmptyOperator(task_id='end', dag=dag)\n",
    "```\n",
    "\n",
    "## 3. Task Dependencies\n",
    "```python\n",
    "task1 >> task2  # task1 runs before task2\n",
    "task3 << task2  # task3 runs after task2\n",
    "\n",
    "[task1, task2] >> task3  # Both task1 and task2 must complete before task3\n",
    "```\n",
    "\n",
    "## 4. XCom for Data Passing Between Tasks\n",
    "### Push Data\n",
    "```python\n",
    "def push_data(**kwargs):\n",
    "    ti = kwargs['ti']\n",
    "    ti.xcom_push(key='my_key', value=42)\n",
    "```\n",
    "### Pull Data\n",
    "```python\n",
    "def pull_data(**kwargs):\n",
    "    ti = kwargs['ti']\n",
    "    value = ti.xcom_pull(task_ids='push_task', key='my_key')\n",
    "    print(f\"Received value: {value}\")\n",
    "```\n",
    "\n",
    "## 5. Branching Logic\n",
    "```python\n",
    "from airflow.operators.python import BranchPythonOperator\n",
    "\n",
    "def choose_branch(**kwargs):\n",
    "    return 'task_A' if random.random() < 0.5 else 'task_B'\n",
    "\n",
    "branch_task = BranchPythonOperator(\n",
    "    task_id='branching',\n",
    "    python_callable=choose_branch,\n",
    "    dag=dag\n",
    ")\n",
    "```\n",
    "\n",
    "## 6. Sensors for External Triggers\n",
    "### TimeSensor\n",
    "```python\n",
    "from airflow.sensors.time_sensor import TimeSensor\n",
    "\n",
    "wait_for_time = TimeSensor(\n",
    "    task_id='wait_until_10am',\n",
    "    target_time='10:00:00',\n",
    "    dag=dag\n",
    ")\n",
    "```\n",
    "### FileSensor\n",
    "```python\n",
    "from airflow.sensors.filesystem import FileSensor\n",
    "\n",
    "wait_for_file = FileSensor(\n",
    "    task_id='wait_for_file',\n",
    "    filepath='/path/to/file.txt',\n",
    "    poke_interval=60,\n",
    "    timeout=600,\n",
    "    dag=dag\n",
    ")\n",
    "```\n",
    "\n",
    "## 7. Triggering Other DAGs\n",
    "```python\n",
    "from airflow.operators.trigger_dagrun import TriggerDagRunOperator\n",
    "\n",
    "trigger = TriggerDagRunOperator(\n",
    "    task_id='trigger_other_dag',\n",
    "    trigger_dag_id='other_dag',\n",
    "    dag=dag\n",
    ")\n",
    "```\n",
    "\n",
    "## 8. Dynamic Task Generation\n",
    "```python\n",
    "task_list = []\n",
    "for i in range(3):\n",
    "    task = PythonOperator(\n",
    "        task_id=f'task_{i}',\n",
    "        python_callable=lambda: print(f\"Task {i}\"),\n",
    "        dag=dag\n",
    "    )\n",
    "    task_list.append(task)\n",
    "\n",
    "task_list[0] >> task_list[1] >> task_list[2]\n",
    "```\n",
    "\n",
    "## 9. Task Failure Handling\n",
    "```python\n",
    "from datetime import timedelta\n",
    "\n",
    "def failure_callback(context):\n",
    "    print(f\"Task {context['task_instance'].task_id} failed\")\n",
    "\n",
    "task = PythonOperator(\n",
    "    task_id='task_with_failure_callback',\n",
    "    python_callable=my_function,\n",
    "    on_failure_callback=failure_callback,\n",
    "    retries=3,\n",
    "    retry_delay=timedelta(minutes=5),\n",
    "    dag=dag\n",
    ")\n",
    "```\n",
    "\n",
    "## 10. Parallel Task Execution\n",
    "```python\n",
    "parallel_task1 = PythonOperator(task_id='parallel_1', python_callable=my_function, dag=dag)\n",
    "parallel_task2 = PythonOperator(task_id='parallel_2', python_callable=my_function, dag=dag)\n",
    "\n",
    "start >> [parallel_task1, parallel_task2] >> end\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322dd5ad-4404-4cc4-b82b-2b77aba65c2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
